# -*- coding: utf-8 -*-
"""BERT_pre-training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l3ZHCyu3UeN96gOpAo1hQhs6t_71mvK-
"""

pip install datasets

pip install transformers

pip install tokenizers

pip install pysentimiento

from pysentimiento.preprocessing import preprocess_tweet

preprocess_tweet('RT @bellahtyrah: I love these tearsðŸ˜‚ðŸ˜‚ðŸ˜‚TACHA OUR BITCOIN#GodMadeTacha' , lang='en')

from datasets import load_dataset
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/tweet_train.csv',lineterminator='\n')

from tqdm.auto import tqdm  # for our loading bar

text_data = []
file_count = 0

for sample in tqdm(df['text']):
    # remove newline characters from each sample as we need to use exclusively as seperators
    sample = sample.replace('\n', '')
    sample = sample.replace('\r', '')
    text_data.append(sample)
    if len(text_data) == 10_000:
        # once we hit the 10K mark, save to file
        with open(f'/content/drive/MyDrive/text/text_{file_count}.txt', 'w', encoding='utf-8') as fp:
            fp.write('\n'.join(text_data))
        text_data = []
        file_count += 1
 #after saving in 10K chunks, we will have ~3808 leftover samples, we save those now too
with open(f'/content/drive/MyDrive/text/text_{file_count}.txt', 'w', encoding='utf-8') as fp:
    fp.write('\n'.join(text_data))

from pathlib import Path
paths = [str(x) for x in Path('/content/drive/MyDrive/text').glob('**/*.txt')]

from tokenizers import ByteLevelBPETokenizer
# initialize
tokenizer = ByteLevelBPETokenizer()
# and train
tokenizer.train(files=paths[0:2], vocab_size=30_522, min_frequency=2,
                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])

import os

os.mkdir('/content/drive/MyDrive/text/token')

tokenizer.save_model('/content/drive/MyDrive/text/token')

from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained('/content/drive/MyDrive/text/token', max_len=512)

# Testing the tokenizer

test = 'RT @bellahtyrah: I love these tearsðŸ˜‚ðŸ˜‚ðŸ˜‚TACHA OUR BITCOIN#GodMadeTacha'
tokenizer(test, max_length=15, padding='max_length', truncation=True)

import torch

def mlm(tensor):
  rand = torch.rand(tensor.shape)
  mask_arr = (rand < 0.15) * (tensor > 2)
  for i in range(tensor.shape[0]):
    selection = torch.flatten(mask_arr[i].nonzero()).tolist()
    tensor[i,selection] = 4
  return(tensor)

from tqdm.auto import tqdm

input_ids = []
mask = []
labels = []

for path in tqdm(paths[:5]):  #for the sake of time, only 5 files have been used for training.
    with open(path, 'r', encoding='utf-8') as fp:
      lines = fp.read().split('\n')
    sample = tokenizer(lines, max_length=512, padding='max_length', truncation=True, return_tensors='pt') 
    labels.append(sample.input_ids)
    mask.append(sample.attention_mask)
    input_ids.append(mlm(sample.input_ids).clone())

input_ids = torch.cat(input_ids)
mask = torch.cat(mask)
labels = torch.cat(labels)

encodings = {
    'input_ids': input_ids,
    'attention_mask': mask,
    'labels': labels
}

class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        # store encodings internally
        self.encodings = encodings

    def __len__(self):
        # return the number of samples
        return self.encodings['input_ids'].shape[0]

    def __getitem__(self, i):
        # return dictionary of input_ids, attention_mask, and labels for index i
        return {key: tensor[i] for key, tensor in self.encodings.items()}

dataset = Dataset(encodings)

dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)

from transformers import RobertaConfig

config = RobertaConfig(
    vocab_size=30_522,  # we align this to the tokenizer vocab_size
    max_position_embeddings=514,
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=6,
    type_vocab_size=1
)

from transformers import RobertaForMaskedLM

model = RobertaForMaskedLM(config)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
# and move our model over to the selected device
model.to(device)

from transformers import AdamW

# activate training mode
model.train()
# initialize optimizer
optim = AdamW(model.parameters(), lr=1e-4)

epochs = 1

for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(dataloader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())